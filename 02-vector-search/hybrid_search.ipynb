{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5424e09a-7697-4e8d-9aa7-9a5f7c698351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollectionsResponse(collections=[CollectionDescription(name='zoomcamp-sparse-dense'), CollectionDescription(name='zoomcamp-sparse'), CollectionDescription(name='zoomcamp-faq')])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "\n",
    "client = QdrantClient(\"http://localhost:6333\")\n",
    "client.get_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8591812-812d-444f-8729-d1ee4c20619d",
   "metadata": {},
   "source": [
    "#### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fce7e1-0dc4-4d16-ba3c-90982e196584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "docs_url = \"https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json\"\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983a160c-c65d-4694-bdbd-c8f2a8e5162c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_raw[0][\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e375c43-279d-41fa-96a8-b41852504a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    for doc in course[\"documents\"]:\n",
    "        doc[\"course\"] = course[\"course\"]\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92d280aa-f411-4614-b4a3-8f31a2b94a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ea08e3d-e21b-41cc-8366-df237153b416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_name = \"zoomcamp-sparse\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e72376c-d587-4bcc-982e-697ca39df674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "points = []\n",
    "\n",
    "for doc in documents:\n",
    "    text = doc[\"question\"] + \" \" + doc[\"text\"]\n",
    "    vector = {\"bm25\": models.Document(text=doc[\"text\"], model=\"Qdrant/bm25\")}\n",
    "    point = models.PointStruct(\n",
    "        id=uuid.uuid4().hex,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95904a54-2482-4029-b290-f202bb59c482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa8b897fe79401fb49eb90065a1d2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254cba0fa87646acae86f29935dbbaac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fad905d8-312d-42b9-9dd2-cae09ed55213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, limit=1):\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\"\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec91db95-9049-4a67-bf52-4fee4236aaf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.442513,\n",
       " {'text': 'If you get an error while running the command python3 stream.py worker\\nRun pip uninstall kafka-python\\nThen run pip install kafka-python==1.4.6\\nWhat is the use of  Redpanda ?\\nRedpanda: Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.\\nRedpanda is a powerful, yet simple, and cost-efficient streaming data platform that is compatible with Kafka® APIs while eliminating Kafka complexity.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Error while running python3 stream.py worker',\n",
       "  'course': 'data-engineering-zoomcamp'})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search(\"How to run Kafka\")[0]\n",
    "result.score, result.payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1da13fdd-931b-46bf-9909-7dddcf0de2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"text\": \"Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\\nYes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\\nPastor Soto\",\n",
      "  \"section\": \"3. Machine Learning for Classification\",\n",
      "  \"question\": \"What data should we use for correlation matrix\",\n",
      "  \"course\": \"machine-learning-zoomcamp\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "course = random.choice(documents_raw)\n",
    "doc = random.choice(course[\"documents\"])\n",
    "print(json.dumps(doc, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c8ac220f-6fe1-4779-914d-c0382c4e325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved:\n",
      " The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\n",
      "Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\n",
      "# ensure to have only numerical values in the dataframe before calling 'corr'\n",
      "corr_mat = df_numerical_only.corr()\n",
      "corr_mat.style.background_gradient(cmap='viridis')\n",
      "Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\n",
      "np.random.seed = 3\n",
      "df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n",
      "df_random.style.background_gradient(cmap='viridis')\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "original:\n",
      " Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n"
     ]
    }
   ],
   "source": [
    "result = search(doc[\"question\"])[0]\n",
    "print(f'retrieved:\\n {result.payload[\"text\"]}', end=\"\\n\\n\")\n",
    "print(f'original:\\n {doc[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c8a62-6ffc-41ad-9737-508ce9f38ccd",
   "metadata": {},
   "source": [
    "#### Prefetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea59049-35d5-4788-a92a-d2e079c54544",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"zoomcamp-sparse-dense\"\n",
    "embedding_dim = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b438ce-47b2-48ce-a022-2ba9d2dbcdfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"jina-small\": models.VectorParams(\n",
    "            size=embedding_dim,\n",
    "            distance=models.Distance.COSINE\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(\n",
    "            modifier=models.Modifier.IDF\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7135290c-9e40-479b-9732-948c1f5345f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "points = []\n",
    "\n",
    "for doc in documents:\n",
    "    text = doc[\"question\"] + \" \" + doc[\"text\"]\n",
    "    vector = {\n",
    "        \"jina-small\": models.Document(text=doc[\"text\"], model=model_handle),\n",
    "        \"bm25\": models.Document(text=doc[\"text\"], model=\"Qdrant/bm25\")\n",
    "    }\n",
    "    point = models.PointStruct(\n",
    "        id=uuid.uuid4().hex,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f511ff4-44cc-403f-8f21-53804b9cbc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9961452f911141e59001056399908e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f58b6132cd244d7b81027cbff963e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e434c59962a74827b37eee1011e9e33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d053022cc6f54708b300a2c1ccbe7fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "563e50cc-3758-4da6-aeef-1357373867af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_stage_search(query, limit=1):\n",
    "    prefetch = [models.Prefetch(\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=model_handle\n",
    "        ),\n",
    "        using=\"jina-small\",\n",
    "        limit=(limit * 10)\n",
    "    )]\n",
    "    \n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=prefetch,\n",
    "        query=models.Document(\n",
    "            text=query,\n",
    "            model=\"Qdrant/bm25\"\n",
    "        ),\n",
    "        using=\"bm25\",\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3e79335-82f0-40bc-9f2d-c74c5f435b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved:\n",
      " The background of any dataframe can be colored (not only the correlation matrix) based on the numerical values the dataframe contains by using the method pandas.io.formats.style.Styler.background_graident.\n",
      "Here an example on how to color the correlation matrix. A color map of choice can get passed, here ‘viridis’ is used.\n",
      "# ensure to have only numerical values in the dataframe before calling 'corr'\n",
      "corr_mat = df_numerical_only.corr()\n",
      "corr_mat.style.background_gradient(cmap='viridis')\n",
      "Here is an example of how the coloring will look like using a dataframe containing random values and applying “background_gradient” to it.\n",
      "np.random.seed = 3\n",
      "df_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n",
      "df_random.style.background_gradient(cmap='viridis')\n",
      "Added by Sylvia Schmitt\n",
      "\n",
      "original:\n",
      " Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n"
     ]
    }
   ],
   "source": [
    "result = multi_stage_search(doc[\"question\"])[0]\n",
    "print(f'retrieved:\\n {result.payload[\"text\"]}', end=\"\\n\\n\")\n",
    "print(f'original:\\n {doc[\"text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd593ac1-3909-4ad1-b2cc-57afd5c4cd51",
   "metadata": {},
   "source": [
    "#### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f34b46f-b6fe-4837-a783-1683b8d18428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_search(query, limit=1):\n",
    "    # Reciprocal Rank Fusion\n",
    "    prefetch = [\n",
    "        models.Prefetch(\n",
    "            query=models.Document(\n",
    "                text=query,\n",
    "                model=model_handle\n",
    "            ),\n",
    "            using=\"jina-small\",\n",
    "            limit=(limit * 5)\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=models.Document(\n",
    "                text=query,\n",
    "                model=\"Qdrant/bm25\"\n",
    "            ),\n",
    "            using=\"bm25\",\n",
    "            limit=(limit * 5)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    results = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=prefetch,\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=limit,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    return results.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bf07b14b-2f4c-43b2-bf30-71df252553eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved:\n",
      " Should correlation be calculated after splitting or before splitting. And lastly I know how to find the correlation but how do i find the two most correlated features.\n",
      "Answer: Correlation matrix of your train dataset. Thus, after splitting. Two most correlated features are the ones having the highest correlation coefficient in terms of absolute values.\n",
      "\n",
      "original:\n",
      " Q2 asks about correlation matrix and converting median_house_value from numeric to binary. Just to make sure here we are only dealing with df_train not df_train_full, right? As the question explicitly mentions the train dataset.\n",
      "Yes. I think it is only on df_train. The reason behind this is that df_train_full also contains the validation dataset, so at this stage we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\n",
      "Pastor Soto\n"
     ]
    }
   ],
   "source": [
    "result = rrf_search(doc[\"question\"])[0]\n",
    "print(f'retrieved:\\n {result.payload[\"text\"]}', end=\"\\n\\n\")\n",
    "print(f'original:\\n {doc[\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573edee1-e7f5-4712-985d-cdaa7a280192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
